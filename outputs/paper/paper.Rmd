---
title: "TBD"
subtitle: "TBD"
author: "Arjun Dhatt, Benjamin Draskovic, Yiqu Ding, Gantavya Gupta"
thanks: "Code and data are available at: https://github.com/STA304-PS4/Trump-vs-Biden."
date: "`r format(Sys.time(), '%d %B %Y')`"
abstract: |
  | First sentence. Second sentence. Third sentence. Fourth sentence. 
output:
  bookdown::pdf_document2:
toc: FALSE
bibliography: references.bib
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
```

# Introduction


# Data


# Model

The model we use to predict the result of the election is MRP, multilevel regression with post-stratification. The advantage of this model is that it has better performance when estimating behaviors of a particular subgroup of the population. One can interpret MRP as a combination of multilevel logistic regression and post-stratification. Logistic regression allows us the simplicity of analyzing the categorical response variable while incorporating both numerical and categorical explanatory variables; while combined with post-stratification, it yields a much narrower confidence interval for the estimation. 

We are interested in how variables such as age, education, race, family income, geographic region, and mask-wearing decision affect citizens’ voting intentions, particularly between the major party candidates, Donald Trump for the Republicans and Joe Biden for the Democratic Party. The response variable that we are interested in is the chances of a citizen voting for Trump in the coming presidential election. It is a categorical variable with two levels: 1 represents intending to vote for Donald Trump, and 2 means planning to vote for Joe Biden.

Ideally, we ought to run a pre-analysis to determine the variables of interest. However, we will leave that open for further analysis and future elections due to the limited time and budget. We chose the explanatory variables based on our understanding of a respondent’s significant characteristics that could potentially affect his/her voting intention. Except for respondents' age, education, income level, and state(common in statistical studies), we are very interested in the respondent's habits towards mask-wearing. Firstly because the US is the country with the most COVID cases globally, the influence of COVID on the electoral votes is inevitable. Secondly, since Trump and Biden hold quite opposites opinions towards handling the pandemic, this parameter will likely represent part of the respondent's voting intention in the same way as the more familiar political indicators. 

First, we train a multilevel logistic regression model using raw data from the voter study group towards the variable of interest using the seven explanatory variables. We then apply this model to our post-stratification data set to get the estimates.

Logistic regression estimates $\beta_0...\beta_k$ in  \@ref(eq:logistic)

\begin{equation}
log(\frac{p}{1-p})=\beta_0+\beta_1x_1+...+\beta_kx_k (\#eq:logistic)
\end{equation}

where p is the probability of event A that we are interested in, $\beta_0$is the intercept, $x_1...x_K$ are our variables of interest and $\beta_1...\beta_k$ are parameters for each of these variables. Based on the result, we are able to estimate p for a particular case given all the variables. We use `as.factors()` to incorporate dummy variables for all the categorical variable. 

The main logic behind post-stratification is that we divide the sample data into strata based on a few attributes such as state, income, education level. Then we come up with a weight for each stratum to adjust its influence towards our prediction. Namely, if a stratum is over-represented, we want to reduce its impact on the prediction result; if a stratum is under-represented, we want to increase its influence. In this sense, the combination with logistic regression allows subgroups with a relatively small size to 'reference' from other subgroups with similar characteristics.

To mimic the population as closely as possible, we need a post-stratification dataset large enough to refer to. In this report, we use the ACS(reason). 

The post-stratification estimate is defined by $\hat{y}_{PS} = \frac{\sum{N_j\hat{y}_j}}{\sum{N_j}}$ where $\hat{y}_j$ is the estimate of y in cell j, and $N_j$ is the size of the j-th cell in the population. An illustration using the education variable is $\hat{y}_{edu}^{PS} = \frac{\sum_{j\in J_{edu}}{N_j\hat{y}_j}}{\sum_{j\in J_{edu}}{N_j}}$, we can get an estimation given any level of education of respondents, $J_{edu}$ denotes all subsets of education levels, $\cup_{i\in J} J^{edu}_{i} =$ all possible education levels. 

\@ref(eq:logistic) produces a proportion for the post-stratification based on regression, instead of merely averaging the sample in each stratum. Specifically, all possible cells are determined from combining:

- 76 different ages;
- 11 education levels
- 15 races;
- 24 income levels;
- 51 states
- 2 options for mask wearing habits.

That is 

We fit the logistic regressions for estimating candidate support in each cell. We used `function` from `package` to fit \@ref(eq:model).

\begin{equation}
P(Trump) = logit^{-1}(
\beta_0 + \beta_age + \beta_{j[i]}^{edu} + \beta_{j[i]}^{race}
+ \beta_{j[i]}^{income} + \beta_{j[i]}^{state} + \beta_{j[i]}^{mask}
) (\#eq:model)
\end{equation}

\@ref(eq:model) is the model that we fit using the survey data, with $\beta_{j[i]}^{var} \sim N(0, \sigma^2_{var})$. 
$\beta_0$ is the intercept,each of $\beta_{j[i]}^{var}$ represents the parameter for the i-th respondent in j-th cell. For example, $\beta_{j[i]}^{state}$ can take values from $\beta$ for all states in the US. Then the model sums up estimates for each cell to the population. 

# Results

```{r, eval = FALSE}
fit <- brms::brm(
  binary ~ (1 | state) + (1 | age) + (1 | education) + (1 | household_income),
  family = bernoulli(link = "logit"),
  data = clean_survey
)
print(fit)
```

```{r, eval=FALSE}
# to get the estimated proportions and associated 95% PIs by state
res_state <- fit %>%
  add_predicted_draws(newdata=state_prop %>% 
                        filter(age_group>20, age_group<80, decade_married>1969),
                      allow_new_levels=TRUE) %>%
  rename(kept_name_predict = .prediction) %>% 
  mutate(kept_name_predict_prop = kept_name_predict*prop) %>% 
  group_by(state_name, .draw) %>% 
  summarise(kept_name_predict = sum(kept_name_predict_prop)) %>% 
  group_by(state_name) %>% 
  summarise(mean = mean(kept_name_predict), 
            lower = quantile(kept_name_predict, 0.025), 
            upper = quantile(kept_name_predict, 0.975))
```

```{r, eval=FALSE}
# graph by state
res_state %>% 
  arrange(-mean) %>% 
  left_join(d %>%
              group_by(state_name, kept_name) %>%
              summarise(n = n()) %>%
              group_by(state_name) %>%
              mutate(prop = n/sum(n)) %>%
              filter(kept_name==1)) %>% 
  ggplot(aes(y = mean, x = forcats::fct_inorder(state_name), color = "MRP estimate")) + 
  geom_point() +
  coord_flip() + 
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + 
  ylab("proportion keeping maiden name") + 
  xlab("state") + 
  geom_point(aes(state_name, prop, color = "raw data")) +
  scale_color_manual(name = "", values = c("MRP estimate" = "black", "raw data" = "red")) + 
  theme_bw() +
  ggtitle("Proportion of women keeping maiden name after marriage")


res_state %>% 
  mutate(statename = str_to_title(state_name)) %>% 
  #mutate(statename = ifelse(state_name=="district of columbia", "District of Columbia", statename)) %>% 
  ggplot(aes(fill = mean, state = statename)) + 
  geom_statebins() +
  scale_fill_viridis_c(name = "proportion keeping name") +
  theme_void()
ggsave("state_plot.png", width = 8, height = 6)
```

# Discussion

## First discussion point

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

## Third discussion point

## Weaknesses and next steps

### 1: Nested Bayesian Model

### 2: Pre-analysis

\newpage

# Appendix {-}

\newpage


# References


